{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Raphael Mourad\n",
    "### Associate Professor\n",
    "### University Paul Sabatier / INRAE MIAT Lab Toulouse\n",
    "### 22/01/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to fine tune mixtral-dna labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mourad/miniconda3/envs/mistral-dna/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.4\n",
      "2.1.0\n",
      "4.36.2\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "### LOAD PYTHON MODULES\n",
    "# Load basic modules\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from os import path\n",
    "import gc\n",
    "\n",
    "# Load data and machine learning modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randrange\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "import torch\n",
    "import triton\n",
    "import transformers\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "from transformers import AutoTokenizer, AutoModel, EarlyStoppingCallback, set_seed, BitsAndBytesConfig\n",
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "# Print numpy version for compatibility with spektral\n",
    "print(np.__version__) # Becareful: numpy should be 1.19 (and not 1.2) for spektral to work!\n",
    "print(triton.__version__)\n",
    "print(transformers.__version__)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mourad/miniconda3/envs/mistral-dna\n"
     ]
    }
   ],
   "source": [
    "### CHECK ENV\n",
    "print(sys.prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/mourad/SSD2/MistralDNA\n"
     ]
    }
   ],
   "source": [
    "### SET DIRECTORY\n",
    "os.chdir(\"/media/mourad/SSD2/MistralDNA\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPECIFY PARAMETERS\n",
    "model_name=\"mixtral-dna\" #  \"DNABERT2\" \"mixtral-dna\"\n",
    "lora=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD FUNCTIONS MODULE\n",
    "sys.path.append(\"scriptPython/\")\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataArguments(data_path=None, kmer=-1)\n",
      "ModelArguments(model_name_or_path='facebook/opt-125m', use_lora=True, lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target_modules='query,value')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "functions.TrainingArguments"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAINING PARAMETERS\n",
    "data_args=DataArguments()\n",
    "print(data_args)\n",
    "\n",
    "model_args=ModelArguments()\n",
    "model_args.use_lora=lora\n",
    "print(model_args)\n",
    "\n",
    "training_args=TrainingArguments\n",
    "training_args.deepspeed_plugin=None\n",
    "training_args.run_name=\"mixtral-dna\"\n",
    "training_args.model_max_length=1024 # max sequence length (can be increased)\n",
    "training_args.gradient_accumulation_steps=1\n",
    "training_args.learning_rate=5e-4\n",
    "training_args.num_train_epochs=10\n",
    "training_args.fp16=True \n",
    "training_args.save_steps=5000\n",
    "training_args.eval_steps=50\n",
    "training_args.evaluation_strategy=\"steps\"\n",
    "training_args.warmup_steps=50\n",
    "training_args.load_best_model_at_end=True\n",
    "training_args.logging_steps=100000\n",
    "training_args.find_unused_parameters=False\n",
    "\n",
    "# Other arguments to add since it was bugging\n",
    "bs=1024\n",
    "training_args.device=torch.device('cuda:0')\n",
    "training_args.report_to=[\"tensorboard\"]\n",
    "training_args.world_size=1\n",
    "#training_args.per_device_train_batch_size=bs\n",
    "training_args.train_batch_size=bs\n",
    "training_args.eval_batch_size=bs\n",
    "training_args.test_batch_size=bs\n",
    "training_args.batch_size=bs\n",
    "training_args.num_training_steps=100\n",
    "training_args.n_gpu=1\n",
    "training_args.distributed_state=None\n",
    "training_args.local_rank=-1 # -1\n",
    "training_args.metric_for_best_model=\"eval_loss\"\n",
    "\n",
    "training_args.fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False, 'xla_device': 'cpu'}\n",
    "training_args.lr_scheduler_kwargs={}\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BitsAndBytesConfig {\n",
       "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
       "  \"bnb_4bit_quant_type\": \"fp4\",\n",
       "  \"bnb_4bit_use_double_quant\": true,\n",
       "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "  \"llm_int8_has_fp16_weight\": false,\n",
       "  \"llm_int8_skip_modules\": null,\n",
       "  \"llm_int8_threshold\": 6.0,\n",
       "  \"load_in_4bit\": true,\n",
       "  \"load_in_8bit\": false,\n",
       "  \"quant_method\": \"bitsandbytes\"\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONFIG QUANTIZATION\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG ACCELERATE\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG LORA\n",
    "peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "WARNING:root:Perform single sequence classification...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splice/reconstructed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Perform single sequence classification...\n",
      "WARNING:root:Perform single sequence classification...\n",
      "loading configuration file results/models/mixtral-dna/config.json\n",
      "Model config MixtralConfig {\n",
      "  \"_name_or_path\": \"results/models/mixtral-dna\",\n",
      "  \"architectures\": [\n",
      "    \"MixtralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 256,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"mixtral\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_experts_per_tok\": 1,\n",
      "  \"num_hidden_layers\": 8,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_local_experts\": 64,\n",
      "  \"output_router_logits\": false,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"router_aux_loss_coef\": 0.02,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.36.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 4096\n",
      "}\n",
      "\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
      "loading weights file results/models/mixtral-dna/model.safetensors\n",
      "Instantiating MixtralForSequenceClassification model under default dtype torch.float16.\n",
      "Detected 4-bit loading: activating 4-bit loading for this model\n",
      "Some weights of the model checkpoint at results/models/mixtral-dna were not used when initializing MixtralForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing MixtralForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MixtralForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MixtralForSequenceClassification were not initialized from the model checkpoint at results/models/mixtral-dna and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using auto half precision backend\n",
      "Currently training with a batch size of: 1024\n",
      "***** Running training *****\n",
      "  Num examples = 36,496\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Training with DataParallel so batch size has been adjusted to: 1,024\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1,024\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 360\n",
      "  Number of trainable parameters = 262,912\n",
      "/home/mourad/miniconda3/envs/mistral-dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/360 00:02 < 13:25, 0.44 it/s, Epoch 0.06/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### CREATE AND TRAIN MODEL\n",
    "\n",
    "# LOOP OVER DATA\n",
    "numRepeats=1\n",
    "\n",
    "expes=[\"tf/0\",\"tf/1\",\"tf/2\",\"tf/3\",\"tf/4\",\n",
    "    \"prom/prom_300_all\",\"prom/prom_300_notata\",\"prom/prom_300_tata\",\n",
    "    \"prom/prom_core_all\",\"prom/prom_core_notata\",\"prom/prom_core_tata\",\n",
    "    \"mouse/0\",\"mouse/1\",\"mouse/2\",\"mouse/3\",\"mouse/4\",\n",
    "    \"EMP/H3\",\"EMP/H3K14ac\",\"EMP/H3K36me3\",\"EMP/H3K4me1\",\n",
    "    \"EMP/H3K4me2\",\"EMP/H3K4me3\",\"EMP/H3K79me3\",\"EMP/H3K9ac\",\"EMP/H4\",\"EMP/H4ac\",\n",
    "    \"splice/reconstructed\",\"virus/covid\"]\n",
    "\n",
    "for expe in expes: \n",
    "    print(expe)\n",
    "    \n",
    "    data_args.data_path=\"data/GUE/\"+expe\n",
    "    model_args.model_name_or_path=\"RaphaelMourad/Mistral-DNA-v0.2\"\n",
    "    training_args.output_dir=\"results/mixtral-dna/GUE/\"+expe+\"/\"\n",
    "    \n",
    "    if expe!=\"splice/reconstructed\":\n",
    "        num_labels=2\n",
    "    else:\n",
    "        num_labels=3\n",
    "    \n",
    "    if os.path.exists(training_args.output_dir)==False:\n",
    "        os.makedirs(training_args.output_dir)\n",
    "\n",
    "    for k in range(numRepeats):\n",
    "\n",
    "        set_seed(randrange(1e8))\n",
    "\n",
    "        # load tokenizer\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            cache_dir=training_args.cache_dir,\n",
    "            model_max_length=training_args.model_max_length,\n",
    "            padding_side=\"right\",\n",
    "            use_fast=True,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        tokenizer.eos_token='[EOS]'\n",
    "        tokenizer.pad_token = '[PAD]'\n",
    "\n",
    "        # define datasets and data collator      \n",
    "        train_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                          data_path=os.path.join(data_args.data_path, \"train.csv\"), \n",
    "                                          kmer=data_args.kmer)\n",
    "        val_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                         data_path=os.path.join(data_args.data_path, \"dev.csv\"), \n",
    "                                         kmer=data_args.kmer)   \n",
    "        test_dataset = SupervisedDataset(tokenizer=tokenizer, \n",
    "                                         data_path=os.path.join(data_args.data_path, \"test.csv\"), \n",
    "                                         kmer=data_args.kmer)        \n",
    "        data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "        # load model\n",
    "        model=transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            cache_dir=training_args.cache_dir,\n",
    "            num_labels=num_labels,\n",
    "            output_hidden_states=False,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map='cuda:0',\n",
    "        )\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model = accelerator.prepare_model(model)\n",
    "\n",
    "        # Setup trainer\n",
    "        trainer = transformers.Trainer(model=model,\n",
    "                                       args=training_args,\n",
    "                                       compute_metrics=compute_metrics,\n",
    "                                       train_dataset=train_dataset, \n",
    "                                       eval_dataset=val_dataset,\n",
    "                                       data_collator=data_collator,\n",
    "                                      callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "                                      )        \n",
    "        trainer.local_rank=training_args.local_rank\n",
    "        trainer.train()\n",
    "\n",
    "        # get the evaluation results from trainer\n",
    "        results_path = training_args.output_dir+\"/metrics\"\n",
    "        results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "        os.makedirs(results_path, exist_ok=True)\n",
    "        with open(os.path.join(results_path, \"test_results_\"+str(k)+\".json\"), \"w\") as f:\n",
    "            json.dump(results, f)\n",
    "\n",
    "        # Save model\n",
    "        trainer.save_state()\n",
    "        \n",
    "        del trainer, model\n",
    "\n",
    "    metrics.to_csv(training_args.output_dir+\"/metrics_auroc_aupr.csv\",index=False)\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral-dna",
   "language": "python",
   "name": "mistral-dna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
